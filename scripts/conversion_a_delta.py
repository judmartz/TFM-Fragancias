import os
from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip

# ğŸš€ Inicializa Spark con soporte para Delta Lake
builder = SparkSession.builder \
    .appName("Trusted Zone Cleaning") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.master", "local[*]") \
    .config("spark.sql.warehouse.dir", "file:///tmp/spark-warehouse") \
    .config("spark.hadoop.fs.defaultFS", "file:///")\
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \

spark = configure_spark_with_delta_pip(builder).getOrCreate()

# ğŸ—‚ï¸ Carpetas de entrada y salida
datasets = {
    "sephora": {
        "input_folder": "data/landing/sephora",
        "output_folder": "data/landing/sephora/delta"
    },
    "ulta": {
        "input_folder": "data/landing/ulta",
        "output_folder": "data/landing/ulta/delta"
    }
}

# ğŸ” Procesar cada carpeta
for nombre, rutas in datasets.items():
    input_folder = rutas["input_folder"]
    output_base = rutas["output_folder"]

    print(f"\nğŸ“¦ Procesando dataset: {nombre}")
    if not os.path.exists(input_folder):
        print(f"âš ï¸  Carpeta no encontrada: {input_folder}")
        continue

    for archivo in os.listdir(input_folder):
        if archivo.endswith(".csv"):
            ruta_csv = os.path.join(input_folder, archivo)
            nombre_base = os.path.splitext(archivo)[0]

            # ğŸ§¼ Reemplazar guiones por guiones bajos
            nombre_base_sanitizado = nombre_base.replace("-", "_")

            ruta_delta = os.path.join(output_base, nombre_base_sanitizado)

            # ğŸ› ï¸ Crear carpeta si no existe
            os.makedirs(ruta_delta, exist_ok=True)

            print(f"ğŸ”„ Convirtiendo: {archivo} â†’ Delta")

            df = spark.read.option("header", True).option("inferSchema", True).csv(ruta_csv)
            df = df.dropna(how="all")  # limpieza bÃ¡sica
            df.write.format("delta").mode("overwrite").save(ruta_delta)

            print(f"âœ… Guardado en: {ruta_delta}")

# âœ… Finaliza
spark.stop()
