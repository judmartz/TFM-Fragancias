import os
import sys
from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip
from pyspark.sql.functions import col, lower

# ğŸ“Œ Validar argumentos
if len(sys.argv) != 3:
    print("âŒ Uso: python trusted_clean_single.py <dataset> <tabla>")
    sys.exit(1)

dataset = sys.argv[1]  # "sephora" o "ulta"
tabla = sys.argv[2]    # ej. reviews_250_500

input_path = f"data/landing/{dataset}/delta/{tabla}"
output_path = f"data/trusted/{dataset}_clean/{tabla}"

# ğŸš€ Iniciar Spark con soporte Delta
builder = SparkSession.builder \
    .appName(f"Cleaning {dataset}/{tabla}") \
    .master("local[*]") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.sql.warehouse.dir", "file:///tmp/spark-warehouse") \
    .config("spark.hadoop.fs.defaultFS", "file:///")\
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \

spark = configure_spark_with_delta_pip(builder).getOrCreate()

try:
    print(f"ğŸ“¥ Leyendo: {input_path}")
    df = spark.read.format("delta").load(input_path)

    print("ğŸ§¼ Aplicando limpieza...")
    df_clean = df.dropna(how="all").dropDuplicates()

    if "review_text" in df_clean.columns:
        df_clean = df_clean.withColumn("review_text", lower(col("review_text")))

    print(f"ğŸ’¾ Guardando limpio en: {output_path}")
    df_clean.write.format("delta").mode("overwrite").save(output_path)

    print(f"âœ… Limpieza completada para {tabla} ({df_clean.count()} filas).")

except Exception as e:
    print(f"âŒ Error procesando {tabla}: {e}")

finally:
    spark.stop()
